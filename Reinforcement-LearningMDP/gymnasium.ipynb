{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:09:13.579440Z","iopub.execute_input":"2025-06-08T17:09:13.579757Z","iopub.status.idle":"2025-06-08T17:09:13.585941Z","shell.execute_reply.started":"2025-06-08T17:09:13.579735Z","shell.execute_reply":"2025-06-08T17:09:13.584888Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import gymnasium as gym\nimport numpy as np\nimport warnings\nfrom gymnasium.envs.registration import register\nfrom gymnasium.envs.toy_text.frozen_lake import generate_random_map\n\n# Ignore unnecessary warnings (like environment plugins)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n\n# 1. ENVIRONMENT SETUP\n\n# Default 4x4 Frozen Lake (slippery)\noriginal_env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n\n# Custom 4x4 Lake (not slippery, fixed map)\ncustom_map = [\n    \"SFFF\",\n    \"FHFH\",\n    \"FFFH\",\n    \"HFFG\"\n]\nregister(\n    id=\"FrozenLake-Custom-v0\",\n    entry_point=\"gymnasium.envs.toy_text:FrozenLakeEnv\",\n    kwargs={\"desc\": custom_map, \"is_slippery\": False},\n)\ncustom_env = gym.make(\"FrozenLake-Custom-v0\")\n\n# Expanded 8x8 Frozen Lake (slippery, random)\nregister(\n    id=\"FrozenLake-Expanded-v0\",\n    entry_point=\"gymnasium.envs.toy_text:FrozenLakeEnv\",\n    kwargs={\"desc\": generate_random_map(size=8), \"is_slippery\": True},\n)\nexpanded_env = gym.make(\"FrozenLake-Expanded-v0\")\n\n\n# 2. VALUE ITERATION\n\ndef value_iteration(env, theta=1e-8, gamma=0.99):\n    \"\"\"Use Bellman Optimality to directly estimate best value function and policy.\"\"\"\n    V = np.zeros(env.observation_space.n)  # Start with zero value for all states\n\n    while True:\n        delta = 0  # Track max change across states\n        for s in range(env.observation_space.n):\n            action_values = np.zeros(env.action_space.n)\n\n            # Estimate value of each action from state s\n            for a in range(env.action_space.n):\n                for prob, next_state, reward, done in env.unwrapped.P[s][a]:\n                    action_values[a] += prob * (reward + gamma * V[next_state])\n\n            best_action_value = np.max(action_values)\n            delta = max(delta, abs(best_action_value - V[s]))\n            V[s] = best_action_value  # Update with best value\n\n        if delta < theta:  # Converged\n            break\n\n    # Derive optimal policy from final value estimates\n    policy = np.zeros(env.observation_space.n, dtype=int)\n    for s in range(env.observation_space.n):\n        action_values = np.zeros(env.action_space.n)\n        for a in range(env.action_space.n):\n            for prob, next_state, reward, done in env.unwrapped.P[s][a]:\n                action_values[a] += prob * (reward + gamma * V[next_state])\n        policy[s] = np.argmax(action_values)\n\n    return policy, V\n\n\n# 3. POLICY ITERATION\n\ndef policy_iteration(env, gamma=0.99):\n    \"\"\"Alternate between evaluating the current policy and improving it.\"\"\"\n    policy = np.random.randint(env.action_space.n, size=env.observation_space.n)\n    V = np.zeros(env.observation_space.n)\n\n    while True:\n        # Policy Evaluation Step\n        while True:\n            delta = 0\n            for s in range(env.observation_space.n):\n                v = V[s]\n                a = policy[s]\n                V[s] = sum([\n                    prob * (reward + gamma * V[next_state])\n                    for prob, next_state, reward, done in env.unwrapped.P[s][a]\n                ])\n                delta = max(delta, abs(v - V[s]))\n            if delta < 1e-8:\n                break\n\n        # Policy Improvement Step\n        stable = True\n        for s in range(env.observation_space.n):\n            old_action = policy[s]\n            action_values = np.zeros(env.action_space.n)\n            for a in range(env.action_space.n):\n                for prob, next_state, reward, done in env.unwrapped.P[s][a]:\n                    action_values[a] += prob * (reward + gamma * V[next_state])\n            best_action = np.argmax(action_values)\n            policy[s] = best_action\n            if old_action != best_action:\n                stable = False\n\n        if stable:\n            break\n\n    return policy, V\n\n\n# 4. POLICY EXECUTION\n\ndef evaluate_policy(env, policy, episodes=100):\n    \"\"\"Test how good the policy is by running it in the environment.\"\"\"\n    total_rewards = []\n    total_steps = []\n\n    for _ in range(episodes):\n        state, _ = env.reset()\n        done = False\n        reward_sum = 0\n        steps = 0\n\n        while not done:\n            state, reward, done, truncated, _ = env.step(policy[state])\n            reward_sum += reward\n            steps += 1\n\n        total_rewards.append(reward_sum)\n        total_steps.append(steps)\n\n    avg_reward = np.mean(total_rewards)\n    avg_steps = np.mean(total_steps)\n    return avg_reward, avg_steps\n\n\n# 5. TESTING ON ALL ENVIRONMENTS\n\ndef evaluate_all(envs, names):\n    \"\"\"Compare both algorithms across all environments.\"\"\"\n    for env in envs:\n        print(f\"\\nðŸ§ª Evaluating: {names[env]}\")\n        for label, algo in [(\"Value Iteration\", value_iteration), (\"Policy Iteration\", policy_iteration)]:\n            policy, _ = algo(env)\n            avg_reward, avg_steps = evaluate_policy(env, policy)\n            print(f\"ðŸ”¹ {label}: Avg Reward = {avg_reward:.2f}, Avg Steps = {avg_steps:.2f}\")\n\n# Run evaluation\nenv_dict = {\n    original_env: \"Original Frozen Lake (4x4 Slippery)\",\n    custom_env: \"Custom Frozen Lake (4x4 Non-Slippery)\",\n    expanded_env: \"Expanded Frozen Lake (8x8 Slippery)\"\n}\nevaluate_all([original_env, custom_env, expanded_env], env_dict)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:09:13.604635Z","iopub.execute_input":"2025-06-08T17:09:13.604961Z","iopub.status.idle":"2025-06-08T17:09:14.516969Z","shell.execute_reply.started":"2025-06-08T17:09:13.604931Z","shell.execute_reply":"2025-06-08T17:09:14.515933Z"}},"outputs":[{"name":"stdout","text":"\nðŸ§ª Evaluating: Original Frozen Lake (4x4 Slippery)\nðŸ”¹ Value Iteration: Avg Reward = 0.84, Avg Steps = 48.12\nðŸ”¹ Policy Iteration: Avg Reward = 0.88, Avg Steps = 43.04\n\nðŸ§ª Evaluating: Custom Frozen Lake (4x4 Non-Slippery)\nðŸ”¹ Value Iteration: Avg Reward = 1.00, Avg Steps = 6.00\nðŸ”¹ Policy Iteration: Avg Reward = 1.00, Avg Steps = 6.00\n\nðŸ§ª Evaluating: Expanded Frozen Lake (8x8 Slippery)\nðŸ”¹ Value Iteration: Avg Reward = 0.20, Avg Steps = 32.96\nðŸ”¹ Policy Iteration: Avg Reward = 0.20, Avg Steps = 30.28\n","output_type":"stream"}],"execution_count":6}]}