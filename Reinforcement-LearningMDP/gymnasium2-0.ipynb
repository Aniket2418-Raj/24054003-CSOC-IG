{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:27:27.474956Z","iopub.execute_input":"2025-06-10T18:27:27.475292Z","iopub.status.idle":"2025-06-10T18:27:27.481014Z","shell.execute_reply.started":"2025-06-10T18:27:27.475268Z","shell.execute_reply":"2025-06-10T18:27:27.480114Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import gymnasium as gym\nimport numpy as np\nimport time\nfrom gymnasium.envs.registration import register, registry\nfrom gymnasium.envs.toy_text.frozen_lake import generate_random_map\n\n\ndef log(message):\n    print(f\"ðŸ”¹ {message}\")\n\n\n#  Core Dynamic Programming: Value Iteration\ndef value_iteration(env, discount=0.99, threshold=1e-8):\n    log(\" Starting Value Iteration...\")\n    start_time = time.time()\n\n    transition_probs = env.unwrapped.P  # Access raw environment transitions\n    num_states = env.observation_space.n\n    num_actions = env.action_space.n\n\n    value_function = np.zeros(num_states)\n    policy = np.zeros(num_states, dtype=int)\n\n    iteration = 0\n    while True:\n        delta = 0\n        for state in range(num_states):\n            action_values = np.zeros(num_actions)\n            for action in range(num_actions):\n                if action not in transition_probs[state]:\n                    continue\n                for prob, next_state, reward, done in transition_probs[state][action]:\n                    action_values[action] += prob * (reward + discount * value_function[next_state])\n            best_action = np.argmax(action_values)\n            delta = max(delta, abs(action_values[best_action] - value_function[state]))\n            value_function[state] = action_values[best_action]\n            policy[state] = best_action\n        iteration += 1\n        if delta < threshold:\n            break\n\n    elapsed = time.time() - start_time\n    log(f\" Converged in {iteration} iterations ( {elapsed:.3f} sec)\")\n    return policy, value_function\n\n\n#  Test a learned policy by running episodes\ndef evaluate_policy(env, policy, episodes=100):\n    total_rewards = []\n    for episode in range(episodes):\n        state, _ = env.reset()\n        done = False\n        reward_sum = 0\n        while not done:\n            action = policy[state]\n            state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            reward_sum += reward\n        total_rewards.append(reward_sum)\n    avg_reward = np.mean(total_rewards)\n    log(f\" Average reward over {episodes} episodes: {avg_reward:.3f}\")\n    return avg_reward\n\n\n#  Register custom environments (with checks)\ndef register_custom_envs():\n    log(\" Setting up custom Frozen Lake environments...\")\n\n    # Remove existing registrations (if re-running)\n    for env_id in [\"CustomFrozenLake-v1\", \"ExpandedFrozenLake-v1\"]:\n        if env_id in registry:\n            del registry[env_id]\n\n    # 5x5 custom map\n    custom_map = generate_random_map(size=5)\n\n    # Custom Frozen Lake with no slipperiness\n    register(\n        id=\"CustomFrozenLake-v1\",\n        entry_point=\"gymnasium.envs.toy_text:FrozenLakeEnv\",\n        kwargs={\"desc\": custom_map, \"is_slippery\": False},\n        max_episode_steps=100,\n    )\n\n    #  Expanded Frozen Lake with a teleport action (action 4)\n    class ExpandedFrozenLake(gym.envs.toy_text.FrozenLakeEnv):\n        def __init__(self, **kwargs):\n            super().__init__(**kwargs)\n            self.action_space = gym.spaces.Discrete(5)  # Original 4 + 1 teleport\n\n            # Add teleport transitions to the transition table (P)\n            for state in self.P:\n                teleport_target = self.np_random.integers(0, self.observation_space.n)\n                self.P[state][4] = [(1.0, teleport_target, 0.0, False)]\n\n        def step(self, action):\n            if action == 4:  # Teleport to random state\n                self.s = self.np_random.integers(0, self.observation_space.n)\n                return self.s, 0.0, False, False, {}\n            return super().step(action)\n\n    # Register the expanded version\n    register(\n        id=\"ExpandedFrozenLake-v1\",\n        entry_point=lambda **kwargs: ExpandedFrozenLake(**kwargs),\n        kwargs={\"desc\": custom_map, \"is_slippery\": False},\n        max_episode_steps=100,\n    )\n\n\n#  Run value iteration across all environments\ndef run_all_environments():\n    register_custom_envs()\n\n    # Setup environments\n    envs = {\n        \"Original FrozenLake\": gym.make(\"FrozenLake-v1\", is_slippery=False),\n        \"Custom FrozenLake\": gym.make(\"CustomFrozenLake-v1\"),\n        \"Expanded FrozenLake\": gym.make(\"ExpandedFrozenLake-v1\"),\n    }\n\n    results = {}\n\n    for env_name, env in envs.items():\n        log(f\"\\n Running Value Iteration on {env_name}\")\n        policy, _ = value_iteration(env)\n        avg_reward = evaluate_policy(env, policy)\n        results[env_name] = avg_reward\n\n    log(\"\\n Summary of All Results:\")\n    for name, reward in results.items():\n        print(f\" {name}: Average Reward = {reward:.3f}\")\n\n\n# Run everything\nif __name__ == \"__main__\":\n    run_all_environments()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:27:27.490934Z","iopub.execute_input":"2025-06-10T18:27:27.491573Z","iopub.status.idle":"2025-06-10T18:27:27.746947Z","shell.execute_reply.started":"2025-06-10T18:27:27.491545Z","shell.execute_reply":"2025-06-10T18:27:27.746232Z"}},"outputs":[{"name":"stdout","text":"ðŸ”¹  Setting up custom Frozen Lake environments...\nðŸ”¹ \n Running Value Iteration on Original FrozenLake\nðŸ”¹  Starting Value Iteration...\nðŸ”¹  Converged in 7 iterations ( 0.002 sec)\nðŸ”¹  Average reward over 100 episodes: 1.000\nðŸ”¹ \n Running Value Iteration on Custom FrozenLake\nðŸ”¹  Starting Value Iteration...\nðŸ”¹  Converged in 9 iterations ( 0.005 sec)\nðŸ”¹  Average reward over 100 episodes: 1.000\nðŸ”¹ \n Running Value Iteration on Expanded FrozenLake\nðŸ”¹  Starting Value Iteration...\nðŸ”¹  Converged in 919 iterations ( 0.171 sec)\nðŸ”¹  Average reward over 100 episodes: 1.000\nðŸ”¹ \n Summary of All Results:\n Original FrozenLake: Average Reward = 1.000\n Custom FrozenLake: Average Reward = 1.000\n Expanded FrozenLake: Average Reward = 1.000\n","output_type":"stream"}],"execution_count":10}]}