{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:59:03.208734Z","iopub.execute_input":"2025-06-10T17:59:03.209360Z","iopub.status.idle":"2025-06-10T17:59:03.216267Z","shell.execute_reply.started":"2025-06-10T17:59:03.209319Z","shell.execute_reply":"2025-06-10T17:59:03.214654Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\nimport gymnasium as gym\nimport numpy as np\nimport time\nfrom gymnasium.envs.registration import register\nfrom gymnasium.envs.toy_text.frozen_lake import generate_random_map\n\n\n# Step 1: Register Custom Environments\n\ncustom_map = [\n    \"SFFH\",\n    \"HFHF\",\n    \"FFFH\",\n    \"HFFG\"\n]  # S: Start, F: Frozen, H: Hole, G: Goal\n\nregister(\n    id=\"FrozenLake-Custom-v1\",\n    entry_point=\"gymnasium.envs.toy_text:FrozenLakeEnv\",\n    kwargs={\"desc\": custom_map, \"is_slippery\": False},\n    max_episode_steps=100,\n)\n\nregister(\n    id=\"FrozenLake-Expanded-v1\",\n    entry_point=\"gymnasium.envs.toy_text:FrozenLakeEnv\",\n    kwargs={\"desc\": generate_random_map(size=8), \"is_slippery\": True},\n    max_episode_steps=200,\n)\n\noriginal_env = gym.make(\"FrozenLake-v1\", is_slippery=True)\ncustom_env = gym.make(\"FrozenLake-Custom-v1\")\nexpanded_env = gym.make(\"FrozenLake-Expanded-v1\")\n\n\n# Step 2: Value Iteration\n\ndef value_iteration(env, gamma=0.99, theta=1e-8):\n    V = np.zeros(env.observation_space.n)\n    while True:\n        delta = 0\n        for s in range(env.observation_space.n):\n            action_values = np.zeros(env.action_space.n)\n            for a in range(env.action_space.n):\n                for prob, next_state, reward, done in env.unwrapped.P[s][a]:\n                    action_values[a] += prob * (reward + gamma * V[next_state])\n            best_value = np.max(action_values)\n            delta = max(delta, abs(best_value - V[s]))\n            V[s] = best_value\n        if delta < theta:\n            break\n\n    policy = np.zeros(env.observation_space.n, dtype=int)\n    for s in range(env.observation_space.n):\n        action_values = np.zeros(env.action_space.n)\n        for a in range(env.action_space.n):\n            for prob, next_state, reward, done in env.unwrapped.P[s][a]:\n                action_values[a] += prob * (reward + gamma * V[next_state])\n        policy[s] = np.argmax(action_values)\n    return policy, V\n\n\n# Step 3: Policy Iteration\n\ndef policy_iteration(env, gamma=0.99):\n    policy = np.random.randint(env.action_space.n, size=env.observation_space.n)\n    V = np.zeros(env.observation_space.n)\n    is_policy_stable = False\n\n    while not is_policy_stable:\n        # Policy Evaluation\n        while True:\n            delta = 0\n            for s in range(env.observation_space.n):\n                v = V[s]\n                a = policy[s]\n                V[s] = sum([\n                    prob * (reward + gamma * V[next_state])\n                    for prob, next_state, reward, done in env.unwrapped.P[s][a]\n                ])\n                delta = max(delta, abs(v - V[s]))\n            if delta < 1e-8:\n                break\n\n        # Policy Improvement\n        is_policy_stable = True\n        for s in range(env.observation_space.n):\n            old_action = policy[s]\n            action_values = np.zeros(env.action_space.n)\n            for a in range(env.action_space.n):\n                for prob, next_state, reward, done in env.unwrapped.P[s][a]:\n                    action_values[a] += prob * (reward + gamma * V[next_state])\n            best_action = np.argmax(action_values)\n            policy[s] = best_action\n            if old_action != best_action:\n                is_policy_stable = False\n\n    return policy, V\n\n\n# Step 4: Evaluate Policy on Episodes\n\ndef evaluate_policy(env, policy, episodes=100):\n    total_rewards = []\n    steps_taken = []\n    for _ in range(episodes):\n        obs, _ = env.reset()\n        done = False\n        total_reward, steps = 0, 0\n        while not done:\n            obs, reward, done, truncated, _ = env.step(policy[obs])\n            total_reward += reward\n            steps += 1\n        total_rewards.append(total_reward)\n        steps_taken.append(steps)\n    return np.mean(total_rewards), np.mean(steps_taken)\n\n\n# Step 5: Compare Performance\n\ndef evaluate_all(envs, names):\n    for env, name in zip(envs, names):\n        print(f\"\\n=== Evaluating {name} ===\")\n        for method_name, algorithm in [\n            (\"Value Iteration\", value_iteration),\n            (\"Policy Iteration\", policy_iteration)\n        ]:\n            start = time.time()\n            policy, _ = algorithm(env)\n            elapsed = time.time() - start\n            avg_reward, avg_steps = evaluate_policy(env, policy)\n            print(f\"{method_name}: Avg Reward = {avg_reward:.2f}, Avg Steps = {avg_steps:.2f}, Time = {elapsed:.4f}s\")\n\n\n# Final Execution\n\nevaluate_all(\n    [original_env, custom_env, expanded_env],\n    [\"Original FrozenLake (4x4, Slippery)\",\n     \"Custom FrozenLake (4x4, Not Slippery)\",\n     \"Expanded FrozenLake (8x8, Slippery)\"]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:59:03.218138Z","iopub.execute_input":"2025-06-10T17:59:03.218806Z","iopub.status.idle":"2025-06-10T17:59:04.206154Z","shell.execute_reply.started":"2025-06-10T17:59:03.218780Z","shell.execute_reply":"2025-06-10T17:59:04.205008Z"}},"outputs":[{"name":"stdout","text":"\n=== Evaluating Original FrozenLake (4x4, Slippery) ===\nValue Iteration: Avg Reward = 0.86, Avg Steps = 49.74, Time = 0.0796s\nPolicy Iteration: Avg Reward = 0.79, Avg Steps = 46.65, Time = 0.0180s\n\n=== Evaluating Custom FrozenLake (4x4, Not Slippery) ===\nValue Iteration: Avg Reward = 1.00, Avg Steps = 6.00, Time = 0.0015s\nPolicy Iteration: Avg Reward = 1.00, Avg Steps = 6.00, Time = 0.0020s\n\n=== Evaluating Expanded FrozenLake (8x8, Slippery) ===\nValue Iteration: Avg Reward = 0.01, Avg Steps = 96.45, Time = 0.2742s\nPolicy Iteration: Avg Reward = 0.02, Avg Steps = 88.02, Time = 0.2889s\n","output_type":"stream"}],"execution_count":6}]}